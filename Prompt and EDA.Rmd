---
title: 'Course project: stage 1'
author: "PSTAT131-231"
output:
  pdf_document:
    latex_engine: xelatex
    extra_dependencies:
    - amsmath
    - xcolor
    - soul
    - amsthm
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      message = F,
                      warning = F,
                      fig.align = 'center',
                      fig.height = 4, 
                      fig.width = 4)

library(pander)
library(tidyverse)
library(ggmap)
library(gridExtra)
library(ggridges)
library(fmsb)
library(magrittr)
library(hrbrthemes)
setwd('C:/Users/David/Desktop/Pstat 131/131 Project')
```
### Project overview and expectations

Your final project will be to merge census data with 2016 voting data to analyze the election outcome. The work will be carried out in two stages:

1. Preparation and planning (guided)
  + Background reading
  + Data preparation
  + Exploratory analysis
  + Tentative plan for statistical modeling
  
2. Data analysis and reporting (open-ended)
  + Statistical modeling
  + Interpretation of results
  + Report findings
  
This document pertains to the first stage: you'll gather background, preprocess and explore the data, and come up with a tentative plan for the second stage. 

Your objective is to work through the steps outlined in this document, which walk you through data preparation and exploration. The structure is similar to a homework assignment, and your deliverable will be a knitted PDF with all steps filled in.

**Formatting guidelines**

* Your knitted document should not include codes.
* All R output should be nicely formatted; plots should be appropriately labeled and sized, and tables should be passed through `pander()`. Raw R output should not be included.
* Avoid displaying extra plots and figures if they don't show information essential to addressing questions.

**Suggestions for teamwork**

* Set a communication plan -- how will you share your work and when/how will you meet?
* Assign roles -- designate a group member to coordinate communication and another group member to coordinate preparation and submission of your deliverables.
* Divide the work! Discuss your skills and interests and assign each group member specific tasks. Many of the tasks can be carried out in parallel. For those that can't, if some of your group members have more immediate availability, have them work on earlier parts, and have other members follow up on their work by completing later parts. 

**Other comments**

* The plan that you lay out at the end of this document is not a firm committment -- you can always shift directions as you get farther along in the project.
* Negative results are okay. Sometimes an analysis doesn't pan out; predictions aren't good, or inference doesn't identify any significant associations or interesting patterns. Please don't feel that the tasks you propose in this first stage need to generate insights; their merit will be assessed not on their outcome but on whether they aim at thoughtful and interesting questions with a reasonable approach.

**Evaluations**

Our main objective at this stage is to position you well to move forward with an analysis of your choosing, and to provide feedback on your proposal. We may suggest course corrections if we spot anything that we anticipate may pose significant challenges downstream, or encourage you to focus in a particular direction when you start your analysis. Our goal is *not* to judge or criticize your ideas, but rather to help make your project a more rewarding experience. Most credit will be tied to simply completing the guided portions. Here are the basic criteria.

* We'll look for the following in the guided portions (Part 0 -- Part 2):
  + Has your group completed each step successfully?
  + Does your document adhere to the formatting guidelines above?
* We'll look for the following in your proposed tasks:
  + Is the task relevant to understanding or predicting the election outcome?
  + Is a clear plan identified for how to prepare the data for statistical modeling that is appropriate for the task?
  + Is the modeling approach sensible given the task?

# Part 0. Background

The U.S. presidential election in 2012 did not come as a surprise. Some correctly predicted the outcome of the election correctly including [Nate Silver](https://en.wikipedia.org/wiki/Nate_Silver), and [many speculated about his approach](https://www.theguardian.com/science/grrlscientist/2012/nov/08/nate-sliver-predict-us-election).

Despite the success in 2012, the 2016 presidential election came as a [big surprise](https://fivethirtyeight.com/features/the-polls-missed-trump-we-asked-pollsters-why/) to many, and it underscored that predicting voter behavior is complicated for many reasons despite the tremendous effort in collecting, analyzing, and understanding many available datasets.

To familiarize yourself with the general problem of predicting election outcomes, read the articles linked above and answer the following questions. Limit your responses to one short paragraph (3-5 sentences) each.

### Question 0 (a)
What makes voter behavior prediction (and thus election forecasting) a hard problem?

One problem in this field is that there is sampling error stemming from selection bias, e.g. a certain pollster might attract more die-hard supporters of either party but miss moderates. Another problem is that polling represents information about intended voting behavior in each state prior to an election. However, this information is not one-to-one with actual voting outcomes on election day. Therefore, how do we reconcile models based on polls with observed voting outcomes? 

### Question 0 (b)
What was unique to Nate Silver's approach in 2012 that allowed him to achieve good predictions?

Bob O’Hara claims that Silver’s approach is to incorporate data as a time series so as to introduce uncertainty onto polling data. Adding uncertainty terms based on time between the poll and election, Silver can simulate outcomes based on the polling data and reconcile this with his model using Bayesian updating. On top of this, Silver aggregated multiple polls together to leverage larger sample sizes. He did not just blindly aggregate polls together, but qualitatively assessed the methodology of each poll, e.g. examining “house effects” against 2008 election results, to give each poll an aggregation weight corresponding to its perceived quality of information.

### Question 0 (c)
What went wrong in 2016? What do you think should be done to make future predictions better?

Even before the general election, Silver underestimated Donald Trump’s chances at securing the Republican nomination--he gave Trump a 2% chance at the nomination. In the general election, Silver gave Trump a 28% chance of winning, higher than other publications, but did not predict a Trump win. What went wrong in 2016 is that the polls had much higher polling error--they may have suffered from selection bias (somewhat fixable with raking) and definitely suffered from a high degree of undecided voters (not fixable) that led poll results to be strongly and misleadingly in favor of Clinton. Trump significantly outperformed polling among non-college-educated whites, so we could incorporate a “demagogue” factor for this demographic that projects demagogues to outperform polls for this group.

\newpage
# Part 1. Datasets

The `project_data.RData` binary file contains three datasets: tract-level 2010 census data, stored as `census`; metadata `census_meta` with variable descriptions and types; and county-level vote tallies from the 2016 election, stored as `election_raw`.
```{r}
load("data/project_data.RData")
```

## Election data

Some example rows of the election data are shown below:
```{r}
filter(election_raw, !is.na(county)) %>% 
  head() %>% 
  pander()
```

The meaning of each column in `election_raw` is self-evident except `fips`. The accronym is short for [Federal Information Processing Standard](https://en.wikipedia.org/wiki/FIPS_county_code). In this dataset, `fips` values denote the area (nationwide, statewide, or countywide) that each row of data represent.

Nationwide and statewide tallies are included as rows in `election_raw` with `county` values of `NA`. There are two kinds of these summary rows:

* Federal-level summary rows have a `fips` value of `US`.
* State-level summary rows have the state name as the `fips` value.

### Question 1 (a)
Inspect rows with `fips == 2000`. Provide a reason for excluding them. 
```{r, include = F}
# scratch work here for inspection -- will not be included in report

x = filter(election_raw, fips == 2000)
```

These rows should be excluded because they are missing the corresponding county information. 

### Question 1 (b)
Drop these observations -- please write over `election_raw` -- and report the data dimensions after removal. 
```{r}
# filter out fips == 2000
election_raw = election_raw[election_raw$fips != 2000,]

# print dimensions
dim(election_raw)
```

## Census data

The first few rows and columns of the `census` data are shown below.
```{r}
census %>% 
  select(1:6) %>% 
  head() %>% 
  pander(digits = 15)
```
Variable descriptions are given in the `metadata` file. The variables shown above are:
```{r}
census_meta %>% head() %>% pander()
```

## Data preprocessing

### Election data

Currently, the election dataframe is a concatenation of observations (rows) on three kinds of observational units: the country (one observation per candidate); the states (fifty-ish observations per candidate); and counties (most observations in the data frame). These are distinguished by the data type of the `fips` value; for the country observations, `fips == US`; for the state observations, `fips` is a character string (the state name); and for the county observations, `fips` is numeric. In general, it's good practice to format data so that each data table contains observations on only one kind of observational unit.

### Question 1 (c)
Separate `election_raw` into separate federal-, state-, and county-level dataframes:

* Store federal-level tallies as `election_federal`.
    
* Store state-level tallies as `election_state`.
    
* Store county-level tallies as `election`. Coerce the `fips` variable to numeric.

```{r}
# create one dataframe per observational unit
election_federal <- election_raw[election_raw$fips == 'US',]

election_state <- election_raw[election_raw$fips !='US',]
election_state <- election_state %>% 
  filter(!is.na(as.character(fips)))

election <- election_raw
election$fips <- as.numeric(election_raw$fips)
election <- election%>%
  na.omit()
```

#### (i) Print the first three rows of `election_federal`. 
Format the table nicely using `pander()`.
```{r}
# print first few rows
election_federal %>%
  head(3) %>%
  pander()
```

#### (ii) Print the first three rows of `election_state`.
Format the table nicely using `pander()`.
```{r}
# print first few rows
election_state %>%
  head(3) %>%
  pander()
```

#### (iii) Print the first three rows of `election`. 
Format the table nicely using `pander()`.
```{r}
# print first few rows
election %>%
  head(3) %>%
  pander()
```

### Census data

The `census` data contains high resolution information (more fine-grained than county-level). In order to align this with the election data, you'll need to aggregate to the county level, which is the highest geographical resolution available in the election data. The following steps will walk you through this process.

### Question 1 (d)
This first set of initial steps aims to clean up the census data and remove variables that are highly correlated. Write a chain of commands to accomplish the following:

  + filter out any rows of `census` with missing values;

  + convert `Men`, `Women`, `Employed`, and `Citizen` to percentages of the total population;

  + drop `Men`, since the percentage of men is redundant (percent men + percent women = 100)

  + compute a `Minority` variable by summing `Hispanic`, `Black`, `Native`, `Asian`, `Pacific` and then remove these variables after creating `Minority`;

  + remove `Income`, `Walk`, `PublicWork`, and `Construction`; and

  + remove variables whose names end with `Err` (standard errors for estimated quantities).
   
Store the result as `census_clean`, and print the first 3 rows and 7 columns. Format the printed rows and columns nicely using `pander()`.

```{r}
## clean census data

# Lists of variables to remove in steps
c <- c('Men','Women', 'Employed', 'Citizen')
d <- c('Hispanic', 'Black', 'Native', 'Asian', 'Pacific')
e <- c('Income', 'Walk', 'PublicWork', 'Construction')

census_clean <- census %>%
  drop_na() %>%
  mutate_at(vars(c),~ 100*.x / TotalPop) %>%
  select(-Men) %>%
  mutate(Minority = rowSums(across(d))) %>%
  select(-d,-e, -ends_with('Err'))

# print first few rows/columns
census_clean[1:3,1:7] %>% pander()
```
 
### Question 1 (e) 
To aggregate the clean census data to the county level, you'll weight the variables by population. Create population weights for sub-county census data by following these steps: 

  + group `census_clean` by `State` and `County`;
  
  + use `add_tally()` to add a `CountyPop` variable with the population; 
  
  + add a population weight variable `pop_wt` computed as `TotalPop/CountyPop` (the proportion of the county population in each census tract);
  
  + multiply all quantitative variables by the population weights (use `mutate(across(..., ~ .x*pop_wt));
  
  + remove the grouping structure (`ungroup()`) and drop the population weights and population variables.

Store the result as `census_clean_weighted`, and print the first 3 rows and 7 columns. Format the output nicely using `pander()`.
```{r}
## compute population-weighted quantitative variables

census_clean_weighted <- census_clean %>%
  group_by(State, County) %>%
  add_tally(TotalPop, name = "CountyPop") %>%
  mutate(pop_wt = TotalPop/CountyPop) %>%
  mutate(across(c(where(is.numeric),-CensusTract), ~.x*pop_wt)) %>%
  ungroup(pop_wt, TotalPop, CountyPop) %>%
  select(-contains("pop"))

# print first few rows/columns
census_clean_weighted[1:3,1:7] %>% pander()
```


### Question 1 (f)
Here you'll aggregate the census data to county level. Follow these steps:

  + group the sub-county data `census_clean_weighted` by state and county;
  
  + compute popluation-weighted averages of each variable by taking the sum of each quantitative variable (use `mutate(across(..., sum))`);
  
  + remove the grouping structure.
    
Store the result as `census_tidy` and print the first 3 rows and 7 columns. Format the output nicely using `pander()`.
```{r}
# aggregate to county level
census_tidy <- census_clean_weighted %>%
  group_by(State, County) %>%
  summarize(across(c(where(is.numeric),-CensusTract), sum)) %>%
  ungroup()

# print first few rows/columns
census_tidy[1:3,1:7] %>% pander()
```

You can check your final result by comparison with the reference dataset in the .Rmd file for this document containing the first 20 rows of the tidy data.
```{r}
load('data/census-tidy-ref.RData')

census_ref[1:3,1:7] %>% pander()
```


### Question 1 (g)
Now that you have tidy versions of the census and election data, and a merged dataset, clear the raw and intermediate dataframes from your environment using `rm(list = setdiff(ls(), ...))`. `ls()` shows all objects in your environment, so the command removes the set difference between all objects and ones that you specify in place of `...`; the latter should be a vector of the object names you want to keep. You should keep the three data frames containing election data for the federal, state, and county levels, and the tidy census data.
```{r}
# clean up environment
list <- c("election", "election_state", "election_federal", "census_tidy")
rm(list=setdiff(ls(), list))
```

\newpage
# Part 2: Exploratory analysis
### Question 2 (a)
How many named presidential candidates were there in the 2016 election? Draw a bar graph of all votes received by each candidate, and order the candidate names by decreasing vote counts. (*Hints*: use the federal-level election data; you may need to log-transform the vote axis to see all the bar heights clearly.)
```{r, fig.height = 10, width = 8}
# plotting codes here
 voting_hist <- ggplot(data = election_federal, mapping = aes(x = reorder(candidate, votes),y = log(votes))) +
  ggtitle("2016 U.S Election\n Vote Count by Candidate") +
  geom_bar(stat="identity", fill="salmon") + 
  labs(x="Candidate Name", y = "Log(Total Votes)")
voting_hist + coord_flip()
```


Next you'll generate maps of the election data using `ggmap`. The .Rmd file for this document contains codes to generate a map of the election winner by state. The codes retrieve state geographical boundaries and merge the geographic data with the statewide winner found from the election data by state.
```{r, eval = F}
# plotting boundaries for US states
states <- map_data("state")
name2abb <- function(statename){
  ix <- match(statename, tolower(state.name))
  out <- state.abb[ix]
  return(out)
}
states <- states %>% 
  mutate(fips = name2abb(region))

# who won each state?
state_winner <- election_state %>% # this line depends on your results above!
  group_by(fips) %>% 
  mutate(total = sum(votes), 
         pct = votes/total) %>% 
  slice_max(pct)

# merge winner with plotting boundaries and make map
left_join(states, state_winner) %>%
  ggplot() + 
  geom_polygon(aes(x = long, 
                   y = lat, 
                   fill = candidate, 
                   group = group), 
               color = "white", 
               size=0.3) +
  coord_fixed(1.3) +
  guides(fill=FALSE) +
  scale_fill_brewer(palette="Set1") +
  theme_nothing()
```


### Question 2 (b) 
Follow the example above to create a map of the election winner by county. The .Rmd file for this document contains codes to get you started.

```{r}
library(maps)
counties <- map_data("county")
counties_fips = county.fips
counties_fips = counties_fips %>%
  mutate(subregion = gsub(".*,", "", polyname),
         region = gsub(",.*", "", polyname))
county = left_join(counties, counties_fips, by = c('region','subregion'))

# who won each county?
county_winner <- election %>% # this line depends on your results above!
  group_by(fips) %>% 
  mutate(total = sum(votes), 
         pct = votes/total) %>% 
  slice_max(pct)

# merge winner with plotting boundaries and make map
left_join(county, county_winner) %>%
  ggplot() + 
  geom_polygon(aes(x = long, 
                   y = lat, 
                   fill = candidate, 
                   group = group), 
               color = "white", 
               size=0.3) +
  coord_fixed(1.3) +
  guides(fill=FALSE) +
  scale_fill_brewer(palette="Set1") +
  theme_nothing()
```
### Question 2 (c)
Which variables drive variation among counties? Carry out PCA for the census data. 

#### (i) Center and scale the data, compute and plot the principal component loadings for the first two PC's.
```{r, fig.height = 5, fig.width = 5}
# center and scale
x_mx <- census_tidy %>%
  select(-State, -County) %>%
  scale(center = T, scale = T)

# compute loadings (matrix v) for PC1 and PC2
v_svd <- svd(x_mx)$v

# plot 
v_svd[, 1:2] %>%
  as.data.frame() %>%
  rename(PC1 = V1, PC2 = V2) %>%
  mutate(Variable = colnames(x_mx)) %>%
  gather(key = 'PC', value = 'Loading', 1:2) %>%
  arrange(Variable) %>%
  ggplot(aes(x = Loading, y = Variable)) +
  geom_point(aes(color = PC)) +
  theme_bw() +
  geom_vline(xintercept = 0, color = 'black') +
  geom_path(aes(group = PC, color = PC)) +
  labs(x = '')
```

#### (ii) Interpret the loading plot. Which variables drive the variation in the data?

#### Answer: 

PC1 will be **large** when `WorkAtHome`, `White`, `Professional`, `IncomePerCap` and `Employed` are **large** and when `Unemployment`, `Service`, `Poverty`, `Minority`, `ChildPoverty`, and `Carpool` are **small**.

PC2 will be **large** when `Production`, `PrivateWork`, `Office`, `MeanCommute`, and `Drive` are **large** and when `OtherTransp`, `Minority`, and `FamilyWork` are **small**.

Given these correlations, we can interpret PC1 as measuring "affluence/high-income" and PC2 as measuring "suburban."

#### (iii) How much total variation is captured by the first two principal components?
```{r, fig.width = 10, echo=F, eval=F}
# scratch work here -- don't show codes or output
pc_vars <- x_svd$d^2/(nrow(x_mx) - 1)

tibble(PC = 1:min(dim(x_mx)),
       Proportion = pc_vars/sum(pc_vars),
       Cumulative = cumsum(Proportion))
```

PC1 and PC2 cumulatively capture $41.44\%$ of total variation.


#### (iv) Plot PC1 against PC2. 
```{r}
## plotting codes here
# compute PCs
z_mx <- x_mx %*% v_svd %>% 
  as.data.frame() %>%
  rename(PC1 = V1, PC2 = V2)
```

```{r, echo = F}
#  Get rows of outliers

PC1_outlier <- boxplot.stats(z_mx$PC1)$out
PC2_outlier <- boxplot.stats(z_mx$PC2)$out

rows <- which(z_mx$PC1 %in% PC1_outlier, z_mx$PC2 %in% PC2_outlier)
outliers <- z_mx[rows,]
```

```{r}
z_mx <- z_mx %>%
  mutate(PC1_PC2 = abs(PC1) + abs(PC2))

PC1_PC2_outlier <- boxplot.stats(z_mx$PC1_PC2)$out
rows_PC1_PC2 <- which(z_mx$PC1_PC2 %in% PC1_PC2_outlier)
outliers_PC1_PC2 <- z_mx[rows_PC1_PC2,]
```

```{r, fig.height = 4, fig.width = 4}
z_mx[, 1:2] %>% 
  ggplot(aes(x = PC1, y = PC2)) +
  geom_point(size = 2) +
  geom_point(data = outliers_PC1_PC2,
             shape = 16, color = 'red',
             size = 2) +
  theme_bw()
```

#### (v) Do you notice any outlier counties? If so, which counties, and why do you think they are outliers?
```{r, echo=F}
# scratch work here
census_tidy[rows,] %>%
  group_by(State, County) %>%
  summarize_all(median) %>%
  select(State, County) %>%
  head(5) %>%
  pander("A Sample of the Outlier Counties")

census_tidy[rows,] %>%
  group_by(State) %>%
  add_tally(name = "Number of Outlier Counties") %>%
  distinct(State, `Number of Outlier Counties`) %>%
  arrange(desc(`Number of Outlier Counties`)) %>%
  pander("Number of Outlier Counties by State/Territory")
```

These are generally poor, rural counties.

They all score low on PC1 and PC2, indicating that these counties have both low affluence and are less suburban.

### Question 2 (d)
Create a visualization of your choice using `census` data. Many exit polls noted that [demographics played a big role in the election](https://fivethirtyeight.com/features/demographics-not-hacking-explain-the-election-results/). If you need a starting point, use [this Washington Post article](https://www.washingtonpost.com/graphics/politics/2016-election/exit-polls/) and [this R graph gallery](https://www.r-graph-gallery.com/) for ideas and inspiration.

#### 538's demographic drivers

We'll examine two factors that 538 said drove election outcome:

* Residents age 25+ with a college degree 
* Non-white residents (`Minority`)

We don't have a direct `Education` variable so we'll use `Professional` and `Office` as  proxy variables.

```{r, echo = F, message = F, fig.width = 10}
p0 <- census_tidy %>% 
  select(Minority, Professional) %>%
  ggplot(aes(x = Minority, y = Professional)) +
  geom_point(size = 0.3, alpha = 0.3, color = '#039399') +
  geom_smooth(method = 'loess', color = '#017075') +
  # Lock scale, hide background
  ylim(0,100) + 
  theme(panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        axis.title.y = element_text(size = 14),
        axis.title.x = element_text(size = 14))

p1 <- census_tidy %>% 
  select(Minority, Office) %>%
  ggplot(aes(x = Minority, y = Office)) +
  geom_point(size = 0.3, alpha = 0.3, color = '#f5a293') +
  geom_smooth(method = 'loess', color = '#d67c6b') +
  # Lock scale, hide background
  ylim(0,100) + 
  theme(panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        axis.title.y = element_text(size = 14),
        axis.title.x = element_text(size = 14))

grid.arrange(p0,p1, ncol = 2)
```

Interestingly, we see a similar pattern of $>90\%$ white counties having lower levels of the education proxy, but education proxy modes when the minority rate is near 15%.

#### UU.S. County Heatmaps

```{r, fig.height = 4, fig.width = 6}

# County coordinates
counties <- read.csv('/Users/laila/Desktop/PSTAT 131/project-stage1/data/county_coord.csv') %>% 
  rename(State = state_name,
         County = county_name) %>% 
  select(County, State, long, lat, group) %>%
  # Match names w/ census_tidy
  mutate(County = str_remove(County, ' County')) %>%
  mutate(County = str_remove(County, ' Parish')) %>%
  mutate(County = ifelse(State == 'Louisiana'  & County == 'La Salle', 'LaSalle', County)) %>%
  mutate(County = replace(County, str_detect(County, 'Dona Ana'), 'Doña Ana')) %>%
  mutate(County = replace(County, str_detect(County, 'Petersburg Census Area'), 'Petersburg Borough')) 

# Join coordinates, values
census_tidy_map <- left_join(census_tidy, counties, by = c('State','County')) 

# Income heatmap
census_tidy_map %>%
  ggplot(aes(long, lat, group = group)) +
  geom_polygon(aes(fill = IncomePerCap), show.legend = T) +
  geom_polygon(
    data = urbnmapr::states, mapping = aes(x = long, y = lat, group = group),
    fill = NA, color = 'black', size = 1
  ) +
  scale_fill_gradient(low = "white", high = "red", na.value = "grey90") +
  coord_map() + # Labels
  ggtitle('Income per capita by US county') +
  theme_void() +
  theme(plot.title = element_text(hjust = 0.5, face = 'bold', size = 22))

# Minority heatmap
census_tidy_map %>%
  ggplot(aes(long, lat, group = group)) +
  geom_polygon(aes(fill = Minority), show.legend = T) +
  geom_polygon(
    data = urbnmapr::states, mapping = aes(x = long, y = lat, group = group),
    fill = NA, color = 'black', size = 1
  ) +
  scale_fill_gradient(low = "white", high = "red", na.value = "grey90") +
  coord_map() + # Labels
  ggtitle('Minority population by US county') +
  theme_void() +
  theme(plot.title = element_text(hjust = 0.5, face = 'bold', size = 22))

# Child poverty heatmap
census_tidy_map %>%
  ggplot(aes(long, lat, group = group)) +
  geom_polygon(aes(fill = ChildPoverty), show.legend = T) +
  geom_polygon(
    data = urbnmapr::states, mapping = aes(x = long, y = lat, group = group),
    fill = NA, color = 'black', size = 1
  ) +
  scale_fill_gradient(low = "white", high = "red", na.value = "grey90") +
  coord_map() + # Labels
  ggtitle('Percentage of children in poverty by US county') +
  theme_void() +
  theme(plot.title = element_text(hjust = 0.5, face = 'bold', size = 22))

# Self-employed heatmap 
census_tidy_map %>%
  ggplot(aes(long, lat, group = group)) +
  geom_polygon(aes(fill = SelfEmployed), show.legend = T) +
  geom_polygon(
    data = urbnmapr::states, mapping = aes(x = long, y = lat, group = group),
    fill = NA, color = 'black', size = 1
  ) +
  scale_fill_gradient(low = "white", high = "red", na.value = "grey90") +
  coord_map() + # Labels
  ggtitle('Percentage of self-employed by US county') +
  theme_void() +
  theme(plot.title = element_text(hjust = 0.5, face = 'bold', size = 22))
```


#### Using ridge plots to examine distributions in 10 mean richest states

```{r, fig.height = 7, fig.width = 10}
states <- census_tidy %>% 
  group_by(State) %>% 
  summarize_all(mean) %>% 
  arrange( desc(IncomePerCap)) %>%
  top_n(5, IncomePerCap) %>%
  select(State) %>%
  unlist()

p0 <- census_tidy %>% 
  filter(State %in% states) %>%
  ggplot(aes(x = IncomePerCap, y = State, fill = State)) +
  geom_density_ridges() +
  guides(fill = FALSE) +
  theme_bw() +
  theme(axis.title.y = element_text(size = 14),
        axis.title.x = element_text(size = 14))

p1 <- census_tidy %>% 
  filter(State %in% states) %>%
  ggplot(aes(x = Professional, y = State, fill = State)) +
  geom_density_ridges()+
  guides(fill = FALSE) +
  theme_bw() + 
  theme(axis.title.y = element_blank(),
        axis.title.x = element_text(size = 14)) 

grid.arrange(p0, p1, ncol =2)
```



```{r, fig.height = 7, fig.width = 7, eval = F, echo = F}

#### Hidden idea because radar charts are very hard to read. 
# Trying a lollipop chart instead

#https://www.r-graph-gallery.com/143-spider-chart-with-saveral-individuals.html

p_CA <- census_tidy %>% 
  filter(State == 'California') %>%
  group_by(State) %>% 
  summarize_all(mean) %>% 
  select_if(~ is.numeric(.) && all(between(., 0, 100)))

p_AL <- census_tidy %>% 
  filter(State == 'Alaska') %>%
  group_by(State) %>% 
  summarize_all(mean) %>% 
  select_if(~ is.numeric(.) && all(between(., 0, 100)))

# Color vector
colors_border=c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) , rgb(0.7,0.5,0.1,0.9) )
colors_in=c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4) , rgb(0.7,0.5,0.1,0.4) )

# Add min, max possible obs
rbind(rep(100,23), rep(0,23), p_CA) %>% 
  as.data.frame() %>%
  radarchart(axistype=1 , 
    #custom polygon
    pcol=colors_border , pfcol=colors_in , plwd=0.5 , plty=1,
    #custom the grid
    cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,100,25), cglwd=0.5,
    #custom labels
    vlcex=0.8
    )

# Add a legend
legend(x=0.7, y=1.3, legend = c('California'), bty = "n", pch=20 , col=colors_in , text.col = "grey", cex=1.2, pt.cex=3)
```


\newpage
# Part 3: Planned work
Now that you've thought about the prediction problem, tidied and explored the census and election data, you should devise a plan for more focused analysis.

Your objective in the second stage of the project is to analyze a merged county-level dataset. The chunk below this paragraph in the .Rmd file for this document combines the vote information for the winning candidate and runner-up in each county with the census data. 
```{r, eval = F}
# define function to coerce state abbreviations to names
abb2name <- function(stateabb){
  ix <- match(stateabb, state.abb)
  out <- tolower(state.name[ix])
  return(out)
}

# top two candidates by county
toptwo <- election %>% 
  group_by(fips) %>% 
  mutate(total = sum(votes), 
         pct = votes/total) %>% 
  slice_max(pct, n = 2)

# create temporary dataframes with matching state/county information
tmpelection <- toptwo %>%
  ungroup %>%
  # coerce names to abbreviations
  mutate(state = abb2name(state)) %>%
  # everything lower case
  mutate(across(c(state, county), tolower)) %>%
  # remove county suffixes
  mutate(county = gsub(" county| columbia| city| parish", 
                       "", 
                       county)) 
tmpcensus <- census_tidy %>% 
  # coerce state and county to lowercase
  mutate(across(c(State, County), tolower))

# merge
merged_data <- tmpelection %>%
  left_join(tmpcensus, 
            by = c("state"="State", "county"="County")) %>% 
  na.omit()

# clear temporary dataframes from environment
rm(list = c('tmpwinner', 'tmpcensus'))

# print first few rows
merged_data[1:4, 1:8] %>% pander()
```

There are a number of possibilities for analyzing this data. Here are just a few:

* Prediction
  + Predict the winner of the popular vote
  + Predict the winner of the general election
  + Predict the winner of each county
  + Predict the vote margin by county
  + Predict the vote margin by state

* Inference
  + Model the probability one candidate wins a county and identify significant associations with census variables
  + Model the vote margin and identify/interpret significant associations with census variables
  + Cluster or group counties and model the probability of a win by one candidate or the vote margin separately for each cluster; look for different patterns of association
  + Model the relationship between votes (or win probabilities) separately for each candidate, and contrast the results.

Each would require some slightly different preprocessing of `merged_data` to select the relevant rows and columns for the specified tasks.

### Question 3
Propose an analysis that you'd like to carry out. Be specific: indicate two tasks you'll pursue and for each task indicate the methods you'll use to approach the task. Your methods description should include mention of how you will prepare `merged_data` for modeling, and which model(s) you'll try.

These descriptions don't need to be long, just enough to convey the general idea. Also, these are not final commitments -- you can always change your mind later on if you like.

#### Task 1

**Task**: For our first task, we plan to predict the winner in each county using demographic information from the census data, including factors such as income per capita, employment  by industry and the prevalence of poverty. 

**Methods**: We hope to use a classification tree method to approach this task. In order to simplify the classification, we plan to drop counties/observations in which third-party candidates won; since there are only 24 observations that record votes for a third-party candidate out of 6,142 in total, we believe that this won't bias our sample. We'll also prepare the `merged_data` by grouping by county and taking the highest percentage in order to make a new dataset with one row for each county that gives the name of the winner and the census characteristics. We'll train a tree on 80 percent of the data to predict the winner. We will also likely use cost-complexity pruning, though we will also try weakest link pruning and compare the predictive power.

#### Task 2

**Task**: We will model the probability one candidate wins a county and identify significant associations with census variables.

**Methods**: We will use generalized linear model methods to explain associations between predictors and class label probabilities and identify important predictors. Furthermore, we will analyze state attributions through visualizations such as ridge charts and the state and county maps. We will also utilize principal components to see correlations between the predictors as well as possibly reduce the dimensions for more accurate results.