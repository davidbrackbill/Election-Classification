---
title: '131 Project Task 1'
subtitle: 'Classification trees'
author: "David Brackbill"
output:
  pdf_document:
    latex_engine: xelatex
    extra_dependencies:
    - amsmath
    - xcolor
    - soul
    - amsthm
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      message = F,
                      warning = F,
                      results = 'hold',
                      fig.align = 'center',
                      fig.height = 4, 
                      fig.width = 4)

library(pander)
library(tidyverse)
library(ggmap)
library(ggthemes)
library(gridExtra)
library(ggridges)
library(fmsb)
library(magrittr)
library(hrbrthemes)
library(modelr)
library(ROCR)
library(tree)
library(maptree)
library(ISLR)
library(rpart)
library(randomForest)
library(gbm)
setwd('C:/Users/David/Desktop/Pstat 131/131 Project')
```

# Import, Tidy

```{r} 
# Import merged_data csv straight from Interim Report
merged_data <- read.csv('C:/Users/David/Desktop/Pstat 131/131 Project/data/merged_data.csv')

# Check import
#merged_data[0:6, 0:8] %>% pander()
```

We want winners of each county and  we also want to drop variables that interfere with the classification tree.

```{r, echo = T}
# Get vote-winner of each county
winner_merged_data <- merged_data %>%
  group_by(fips) %>%
  top_n(1, pct) %>%
  rename(winner = candidate) %>%
  mutate(winner = factor(winner)) %>%
  ungroup %>% 
  select(-c(fips, votes, pct))            #Drop problematic variables

#winner_merged_data[0:3,0:8] %>% pander()
```

## Considerations

```{r}
winner_merged_data %>% group_by(winner) %>% count()
```

There are almost 6 times as many Trump-voting counties as Clinton-voting counties. 

As a result, un-weighted partitioning might not make sense for this problem.

# Classification: Winner prediction by county

## Lab pre-sets

Our initial goal is to predictively classify the winner in each county using a decision tree. We'll start with the vanilla classification model from lab.

### Features

We'll be classifying the winner (factor `winner`) based on the rest of the features. What does the feature space look like?

```{r}
feature_space <- winner_merged_data %>% colnames()
feature_space <- feature_space[-2]

feature_space
noquote(paste('p = ', length(feature_space)))
``` 


### Partition

We'll reuse the options from Lab 4 to partition the data set, grow the tree, and prune the tree.

```{r, echo = T}
# hold out 20% of data as a test set
set.seed(12521)
winner_part <- resample_partition(winner_merged_data, c(test = 0.2, train = 0.8))
train <- as_tibble(winner_part$train)
test <- as_tibble(winner_part$test)
```

### Prune large tree

```{r, echo = T}
# grow a large tree to prune
nmin <- 2
tree_opts <- tree.control(nobs = nrow(train), 
                          minsize = nmin,
                          mindev = exp(-8))
t_0 <- tree(winner ~ ., data = train,
                control = tree_opts, split = 'deviance') 

# cost-complexity pruning
nfolds <- 8
cv_out <- cv.tree(t_0, K = nfolds)

# convert to tibble
cv_df <- tibble(alpha = cv_out$k,
                impurity = cv_out$dev,
                size = cv_out$size)

# choose optimal alpha
best_alpha <- slice_min(cv_df, impurity) %>%
  slice_min(size)

# select final tree
t_opt <- prune.tree(t_0, k = best_alpha$alpha)
summary(t_opt)

# Save this tree and partition for later
t_12521 <- t_opt 
winner_part_12521 <- winner_part
train_12521 <- train
test_12521 <- test
clases_12521 <- classes_train
```

```{r, fig.height = 5, fig.width = 10}
# plot
draw.tree(t_opt, cex = 0.7, size = 2.5, digits = 2)

noquote('Training Errors:')
preds_train_topt <- predict(t_opt, winner_part$train, type = 'class')

classes_train <- as.data.frame(winner_part$train) %>% pull(winner)
training_errors <- table(class = classes_train, pred = preds_train_topt)
training_errors/rowSums(training_errors)

noquote('')

noquote('Testing Errors:')
preds_test_topt <- predict(t_opt, winner_part$test, type = 'class')

classes_test <- as.data.frame(winner_part$test) %>% pull(winner)
test_errors <- table(class = classes_test, pred = preds_test_topt)
vanilla_test_errors <- test_errors /rowSums(test_errors)
vanilla_test_errors

# Save classes for later
classes_12521 <- classes_train
```

### Results

We see a very strong classification rate for Trump but an extremely weak classification rate for Clinton. Further iterations should bring their classification errors closer together.

### EDA

When set.seed(12521), the pruned model used 6 variables:

`White`, `Women`, `Transit`, `Professional`, `total`, and `Production`

How are these variables distributed?

```{r, fig.height = 6, fig.width = 8}
hist <- geom_histogram(bins = 20, color = 'gray80')
dens <- geom_density(bw = 8, color = 'red')

# White
White_gg <- winner_merged_data %>% 
  ggplot(aes(x = White, y = ..density..)) + 
  hist +
  dens +
  theme_minimal() + 
  labs(title = 'Density of White', x='', y='')

# Women
Women_gg <- winner_merged_data %>% 
  ggplot(aes(x = Women, y = ..density..)) + 
  hist +
  dens +
  theme_minimal() +
  labs(title = 'Density of Women', x='', y='')

# Transit
Transit_gg <- winner_merged_data %>% 
  ggplot(aes(x = Transit, y = ..density..)) + 
  hist +
  dens +
  theme_minimal() +
  labs(title = 'Density of Transit', x='', y='')

# Professional
Professional_gg <- winner_merged_data %>% 
  ggplot(aes(x = Professional, y = ..density..)) + 
  hist +
  dens +
  theme_minimal() +
  labs(title = 'Density of Professional', x='', y='')

# total
total_gg <- winner_merged_data %>% 
  ggplot(aes(x = total, y = ..density..)) + 
  hist +
  geom_density(bw = 1, color = 'red') +
  scale_x_log10() + 
  theme_minimal() +
  labs(title = 'Density of total', x='', y='') 

# Production
Production_gg <- winner_merged_data %>% 
  ggplot(aes(x = Production, y = ..density..)) + 
  hist +
  dens +
  theme_minimal() +
  labs(title = 'Density of Production', x='', y='')

grid.arrange(White_gg, Women_gg, Transit_gg, 
             Professional_gg, total_gg, Production_gg, 
             ncol = 3)

```

Something seems wrong with `transit`, which should have a more normal distribution. 

```{r, echo = T}
# Transit range vs mean
winner_merged_data$Transit %>% range()
winner_merged_data$Transit %>% mean()
```

Upon further comparison of census_ref and census_tidy, our `transit` variable is correct. It seems that transit participation is just very low in the US.

## Examining seed effect

How do our results vary based on the randomized seed?

We'll try out 30 more iterations with different seeds.

### Re-run 30 times

```{r, eval = F}
S <- 5
Seed_Errors <- rep(NULL, S)
Seed_Variables <- rep(NULL, S)

for(i in 1:S){
  
  set.seed(i)
  # hold out 20% of data as a test set
  winner_part <- resample_partition(winner_merged_data, c(test = 0.2, train = 0.8))
  train <- as_tibble(winner_part$train)
  test <- as_tibble(winner_part$test)
  
  # grow a large tree to prune
  nmin <- 2
  tree_opts <- tree.control(nobs = nrow(train), 
                            minsize = nmin,
                            mindev = exp(-8))
  t_0 <- tree(winner ~ ., data = train,
                  control = tree_opts, split = 'deviance') 
  
  # cost-complexity pruning
  nfolds <- 8
  cv_out <- cv.tree(t_0, K = nfolds)
  
  # convert to tibble
  cv_df <- tibble(alpha = cv_out$k,
                  impurity = cv_out$dev,
                  size = cv_out$size)
  
  # choose optimal alpha
  best_alpha <- slice_min(cv_df, impurity) %>%
    slice_min(size)
  
  # select final tree
  t_opt <- prune.tree(t_0, k = best_alpha$alpha)
  
  # training errors
  preds_train_topt <- predict(t_opt, winner_part$train, type = 'class')
  
  classes_train <- as.data.frame(winner_part$train) %>% pull(winner)
  training_errors <- table(class = classes_train, pred = preds_train_topt)
  
  # Append to output vectors
  Seed_Errors[i][[1]] <- diag(as.matrix(
    training_errors/rowSums(training_errors)
    ))
  
  Seed_Variables[i][[1]] <- summary(t_opt)$used
}
```

```{r, eval = F}
noquote('Classification success rates:')
Seed_Errors %>% pander()

noquote('Variables used in every tree:')
Reduce(intersect, Seed_Variables)
```


### Results

We see that, regardless of seed, classification of red counties far outperforms classification of blue counties.

Notably, our highest values of blue classification come with lowered values of red classification, signalling that finding and implementing Youden's statistic could improve forecasts for blue counties.

We also see that `Transit`, `White` and `total` were used in all 30 trees.

Because seed variability strongly affects the tree performance, in order to say we have improved our decision tree we have to get quite **strong** results that are **robust** against seed-setting.

## Youden's statistic

```{r, echo = T}
# Get initial tree and partition
t_opt <- t_12521
winner_part <- winner_part_12521 
train <- train_12521
test <- test_12521 
classes_train <- classes_12521

# Calculate Youden's statistic on first pruned tree
probs_train <- predict(t_opt, newdata = train, type = 'vector')

topt_prediction <- prediction(predictions = probs_train[, 2],
 labels = classes_train)

topt_perf <- performance(topt_prediction, 'tpr', 'fpr')

rate_df <- tibble(fpr = topt_perf@x.values[[1]],
                  tpr = topt_perf@y.values[[1]],
                  thresh = topt_perf@alpha.values[[1]]) %>%
  mutate(youden = tpr - fpr)

# Store new threshold
optimal_thresh <- slice_max(rate_df, youden)

optimal_thresh
rate_df
```
```{r, echo = T}
# class probabilities on test partition
probs_test <- predict(t_opt, newdata = test, type = 'vector')

# predicted class labels
preds_test <- factor(probs_test[,2] 
                     >= optimal_thresh$thresh, 
                     labels = colnames(probs_test))

noquote('Testing errors with optimal threshold:')
classes_test <- as.data.frame(test) %>% pull(winner)
test_errors <- table(class = classes_test, pred = preds_test)
test_errors / rowSums(test_errors)

noquote('Testing errors with automatic threshold:')
vanilla_test_errors
```

Why are results the same?

## Gini loss

We'll prune a large tree again, this time replacing the deviance loss function with the Gini loss function.

### Missingness

The Gini loss function mandates that there are no missing values. Do we have any missingness?

```{r, echo = T}
sum(is.na(winner_merged_data))
```

No.

### Prune large tree

We need to use the `rpart` and `rattle` libraries for this step because `tree` was giving us missing value errors when trying to set split = 'gini'.

```{r, fig.height = 12, fig.width = 12}
# hold out 20% of data as a test set
set.seed(12521)
winner_part <- resample_partition(winner_merged_data, c(test = 0.2, train = 0.8))
train <- as_tibble(winner_part$train)
test <- as_tibble(winner_part$test)

# Remove county so predict() works on test data later
train <-  train %>% as.data.frame() %>% select(-county)
test <- test %>% as.data.frame() %>% select(-county)

# grow a large tree
tree_opts <- rpart.control(minbucket = 2)
t_opt <- rpart(winner~., data = train,
             control = tree_opts, 
             parms = list(split = 'gini')) 

# Get best complexity parameter, prune
min_cp <-  t_opt$cptable[which.min(t_opt$cptable[,"xerror"]),"CP"]
t_prune <- prune(t_opt, cp = min_cp)
```

```{r}
## Errors using Gini loss formula, unpruned
noquote('Unpruned Training Errors:')
preds_train_topt <- predict(t_opt, train, type = 'class')

classes_train <- as.data.frame(train) %>% pull(winner)
training_errors <- table(class = classes_train, pred = preds_train_topt)
training_errors/rowSums(training_errors)

noquote('')

noquote('Unpruned Testing Errors:')
preds_test_topt <- predict(t_opt, test, type = 'class')

classes_test <- as.data.frame(test) %>% pull(winner)
test_errors <- table(class = classes_test, pred = preds_test_topt)
vanilla_test_errors <- test_errors /rowSums(test_errors)
vanilla_test_errors

noquote('')

## Errors using Gini loss formula, pruned
noquote('Pruned Training Errors:')
preds_train_topt <- predict(t_prune, train, type = 'class')

training_errors <- table(class = classes_train, pred = preds_train_topt)
training_errors/rowSums(training_errors)

noquote('')

noquote('Pruned Testing Errors:')
preds_test_topt <- predict(t_prune, test, type = 'class')

test_errors <- table(class = classes_test, pred = preds_test_topt)
vanilla_test_errors <- test_errors /rowSums(test_errors)
vanilla_test_errors
```

### Results

We see better classification accuracy from the unpruned tree using Gini loss formula.

Comparing the unpruned Gini tree to the pruned 'deviance' tree, we see __.

Overall, we don't see significant benefits from using the Gini loss formula.

## Hyperparameters

Can we obtain better prediction by tuning the hyperparameters of the original pruned model we created?

Ideas: 

* https://www.guru99.com/r-decision-trees.html#7

* Make a set of 5 values for each parameter and try all combinations?

### K-Folds

Which k-fold validation gives us the best result for the seed?

*Aside* Is there a way to easily test model performance over a set of seeds??

```{r}
S <- 10
errors <- rep(NULL, S)
folds <- rep(NULL, S)

for(i in 1:S){
  
  set.seed(12521)
  # hold out 20% of data as a test set
  winner_part <- resample_partition(winner_merged_data, c(test = 0.2, train = 0.8))
  train <- as_tibble(winner_part$train)
  test <- as_tibble(winner_part$test)
  
  # grow a large tree to prune
  nmin <- 2
  tree_opts <- tree.control(nobs = nrow(train), 
                            minsize = nmin,
                            mindev = exp(-8))
  t_0 <- tree(winner ~ ., data = train,
                  control = tree_opts, split = 'deviance') 
  
  # cost-complexity pruning
  nfolds <- 1 + i
  cv_out <- cv.tree(t_0, K = nfolds)
  
  # convert to tibble
  cv_df <- tibble(alpha = cv_out$k,
                  impurity = cv_out$dev,
                  size = cv_out$size)
  
  # choose optimal alpha
  best_alpha <- slice_min(cv_df, impurity) %>%
    slice_min(size)
  
  # select final tree
  t_opt <- prune.tree(t_0, k = best_alpha$alpha)
  
  # training errors
  preds_train_topt <- predict(t_opt, winner_part$train, type = 'class')
  
  classes_train <- as.data.frame(winner_part$train) %>% pull(winner)
  training_errors <- table(class = classes_train, pred = preds_train_topt)
  
  # Append to output vectors
  errors[i][[1]] <- diag(as.matrix(
    training_errors/rowSums(training_errors)
    ))
}
```
```{r}
errors
```

Doesn't seem like any one obvious choice, although 3/4/5 seem to be the best.

### nMin

```{r}
S <- 10
errors <- rep(NULL, S)
nused <- rep(NULL, S)

for(i in 1:S){
  
  set.seed(12521)
  # hold out 20% of data as a test set
  winner_part <- resample_partition(winner_merged_data, c(test = 0.2, train = 0.8))
  train <- as_tibble(winner_part$train)
  test <- as_tibble(winner_part$test)
  
  # grow a large tree to prune
  nmin <- i
  tree_opts <- tree.control(nobs = nrow(train), 
                            minsize = nmin,
                            mindev = exp(-8))
  t_0 <- tree(winner ~ ., data = train,
                  control = tree_opts, split = 'deviance') 
  
  # cost-complexity pruning
  nfolds <- 5
  cv_out <- cv.tree(t_0, K = nfolds)
  
  # convert to tibble
  cv_df <- tibble(alpha = cv_out$k,
                  impurity = cv_out$dev,
                  size = cv_out$size)
  
  # choose optimal alpha
  best_alpha <- slice_min(cv_df, impurity) %>%
    slice_min(size)
  
  # select final tree
  t_opt <- prune.tree(t_0, k = best_alpha$alpha)
  
  # training errors
  preds_train_topt <- predict(t_opt, winner_part$train, type = 'class')
  
  classes_train <- as.data.frame(winner_part$train) %>% pull(winner)
  training_errors <- table(class = classes_train, pred = preds_train_topt)
  
  # Append to output vectors
  errors[i][[1]] <- diag(as.matrix(
    training_errors/rowSums(training_errors)
    ))
  
  nused[i] <- nmin
}
```
```{r}
errors %>% head(3)
```

Has no effect.

### Grid search

Source: https://www.jeremyjordan.me/hyperparameter-tuning/#:~:text=Grid%20search%20is%20arguably%20the,which%20produces%20the%20best%20results.

## Further optimizations

Classifying on unbalanced data is hard. How can we improve this decision tree model?

* [General ideas](https://stats.stackexchange.com/questions/28029/training-a-decision-tree-against-unbalanced-data)

* Transform feature space 
  - Scale, normalize (White, Transit are skewed and total can be normalized)
    - This does nothing
  - Feature creation/selection

### Normalize features

```{r}
winner_scaled_data <- winner_merged_data %>% 
  mutate_at(c(4:26), funs(c(scale(.))))
```

#### Pruned tree

```{r}
# hold out 20% of data as a test set
set.seed(12521)
winner_part <- resample_partition(winner_scaled_data, c(test = 0.2, train = 0.8))
train <- as_tibble(winner_part$train)
test <- as_tibble(winner_part$test)

# grow a large tree to prune
nmin <- 2
tree_opts <- tree.control(nobs = nrow(train), 
                          minsize = nmin,
                          mindev = exp(-8))
t_0 <- tree(winner ~ ., data = train,
                control = tree_opts, split = 'deviance') 

# cost-complexity pruning
nfolds <- 8
cv_out <- cv.tree(t_0, K = nfolds)

# convert to tibble
cv_df <- tibble(alpha = cv_out$k,
                impurity = cv_out$dev,
                size = cv_out$size)

# choose optimal alpha
best_alpha <- slice_min(cv_df, impurity) %>%
  slice_min(size)

# select final tree
t_opt <- prune.tree(t_0, k = best_alpha$alpha)
```
```{r, fig.height = 5, fig.width = 10}
noquote('Training Errors:')
preds_train_topt <- predict(t_opt, winner_part$train, type = 'class')

classes_train <- as.data.frame(winner_part$train) %>% pull(winner)
training_errors <- table(class = classes_train, pred = preds_train_topt)
training_errors/rowSums(training_errors)

noquote('')

noquote('Testing Errors:')
preds_test_topt <- predict(t_opt, winner_part$test, type = 'class')

classes_test <- as.data.frame(winner_part$test) %>% pull(winner)
test_errors <- table(class = classes_test, pred = preds_test_topt)
vanilla_test_errors <- test_errors /rowSums(test_errors)
vanilla_test_errors
```

#### Results

This does nothing lol.
